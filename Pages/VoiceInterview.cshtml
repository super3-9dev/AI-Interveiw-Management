@page
@model InterviewBot.Pages.VoiceInterviewModel
@using Microsoft.AspNetCore.Mvc.Localization
@inject IViewLocalizer Localizer
@{
    ViewData["Title"] = Localizer["Voice Interview"];
    var currentCulture = HttpContext.Request.Query["culture"].ToString();
    if (string.IsNullOrEmpty(currentCulture))
    {
        currentCulture = HttpContext.Request.Cookies["culture"] ?? "en";
    }
}

<div class="voice-interview-container">
    <!-- Interview Header -->
    <div class="interview-header">
        <div class="header-content">
            <div class="interview-info">
                <h1 class="interview-title">
                    <i class="bi bi-mic"></i>
                    @Model.InterviewTopic
                </h1>
                <p class="interview-subtitle">AI-powered voice interview with real-time feedback</p>
            </div>
            <div class="interview-status">
                <span class="status-badge status-active">In Progress</span>
                <!-- Real-time User Status Graph -->
                <div class="user-status-display">
                    <div class="status-graph-container">
                        <canvas id="statusGraph" width="120" height="40"></canvas>
                    </div>
                    <div class="status-indicators">
                        <span id="userStatus" class="status-text">Ready</span>
                        <div class="status-dot" id="statusDot"></div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- Voice Chat Area -->
    <div class="voice-chat-container">
        <div class="voice-messages" id="voiceMessages">
            <!-- Messages will appear here when interview starts -->
        </div>
    </div>

    <!-- Voice Input Area -->
    <div class="voice-input-container">
        <div class="voice-input-wrapper">
            <div class="voice-control-panel">
                <button class="voice-record-btn" id="recordButton" onclick="toggleRecording()">
                    <i class="bi bi-mic-fill"></i>
                    <span class="record-text">Start Recording</span>
                </button>
                <div class="recording-indicator" id="recordingIndicator" style="display: none;">
                    <div class="pulse-dot"></div>
                    <span>Recording...</span>
                </div>
            </div>

            <div class="voice-transcription">
                <textarea id="transcriptionArea" class="transcription-textarea"
                    placeholder="Your voice will be transcribed here..." readonly></textarea>
                <div class="transcription-actions">
                    <button class="btn btn-primary" id="sendVoiceBtn" onclick="sendVoiceMessage()">
                        <i class="bi bi-send"></i> Send
                    </button>
                    <button class="btn btn-outline-secondary" onclick="clearTranscription()">
                        <i class="bi bi-trash"></i> Clear
                    </button>
                </div>
            </div>
        </div>
    </div>

    <!-- Action Buttons -->
    <div class="action-buttons">
        <a href="/Dashboard@(!string.IsNullOrEmpty(currentCulture) ? $"?culture={currentCulture}" : "")"
            class="btn btn-outline-secondary action-btn">
            <i class="bi bi-arrow-left"></i>
            Back to Dashboard
        </a>
        <button class="btn btn-danger action-btn" onclick="finishInterview()">
            <i class="bi bi-arrow-left"></i>
            Finish Interview
        </button>
    </div>
</div>

<style>
    .voice-interview-container {
        display: flex;
        flex-direction: column;
        height: 100vh;
        background-color: #f8f9fa;
    }

    /* Interview Header */
    .interview-header {
        background: linear-gradient(135deg, #10b981 0%, #059669 100%);
        color: white;
        padding: 1.5rem 2rem;
        box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
    }

    .header-content {
        display: flex;
        justify-content: space-between;
        align-items: center;
        max-width: 1200px;
        margin: 0 auto;
    }

    .interview-info h1 {
        margin: 0;
        font-size: 1.75rem;
        font-weight: 600;
        display: flex;
        align-items: center;
        gap: 0.75rem;
    }

    .interview-subtitle {
        margin: 0.5rem 0 0 0;
        opacity: 0.9;
        font-size: 1rem;
    }

    .status-badge {
        padding: 0.5rem 1rem;
        border-radius: 2rem;
        font-size: 0.875rem;
        font-weight: 500;
    }

    .status-active {
        background-color: rgba(34, 197, 94, 0.2);
        color: #22c55e;
        border: 1px solid rgba(34, 197, 94, 0.3);
    }

    /* Real-time User Status Display */
    .user-status-display {
        display: flex;
        align-items: center;
        gap: 1rem;
        margin-left: 1rem;
    }

    .status-graph-container {
        background-color: rgba(255, 255, 255, 0.1);
        border-radius: 8px;
        padding: 0.5rem;
        backdrop-filter: blur(10px);
    }

    #statusGraph {
        border-radius: 4px;
        background-color: rgba(0, 0, 0, 0.1);
    }

    .status-indicators {
        display: flex;
        flex-direction: column;
        align-items: center;
        gap: 0.25rem;
    }

    .status-text {
        font-size: 0.75rem;
        font-weight: 500;
        color: rgba(255, 255, 255, 0.9);
    }

    .status-dot {
        width: 8px;
        height: 8px;
        border-radius: 50%;
        background-color: #22c55e;
        transition: all 0.3s ease;
    }

    .status-dot.speaking {
        background-color: #ef4444;
        animation: pulse 1.5s infinite;
    }

    .status-dot.listening {
        background-color: #3b82f6;
        animation: pulse 1.5s infinite;
    }

    .status-dot.processing {
        background-color: #f59e0b;
        animation: pulse 1.5s infinite;
    }

    /* Typing indicator */
    .typing-indicator {
        display: inline-flex;
        align-items: center;
        gap: 4px;
    }

    .typing-indicator span {
        width: 8px;
        height: 8px;
        border-radius: 50%;
        background-color: #6c757d;
        animation: typing 1.4s infinite ease-in-out;
    }

    .typing-indicator span:nth-child(1) {
        animation-delay: -0.32s;
    }

    .typing-indicator span:nth-child(2) {
        animation-delay: -0.16s;
    }

    @@keyframes typing {
        0%, 80%, 100% {
            transform: scale(0.8);
            opacity: 0.5;
        }
        40% {
            transform: scale(1);
            opacity: 1;
        }
    }

    /* Voice Chat Container */
    .voice-chat-container {
        flex: 1;
        overflow-y: auto;
        padding: 2rem;
        max-width: 1200px;
        margin: 0 auto;
        width: 100%;
    }

    .voice-messages {
        display: flex;
        flex-direction: column;
        gap: 1.5rem;
    }

    /* Voice Message Styles */
    .voice-message {
        display: flex;
        gap: 1rem;
        max-width: 80%;
    }

    .ai-voice-message {
        align-self: flex-start;
    }

    .user-voice-message {
        align-self: flex-end;
        flex-direction: row-reverse;
    }

    .voice-avatar {
        width: 40px;
        height: 40px;
        border-radius: 50%;
        display: flex;
        align-items: center;
        justify-content: center;
        flex-shrink: 0;
    }

    .ai-voice-message .voice-avatar {
        background-color: #10b981;
        color: white;
    }

    .user-voice-message .voice-avatar {
        background-color: #3b82f6;
        color: white;
    }

    .voice-avatar i {
        font-size: 1.2rem;
    }

    .voice-content {
        background-color: white;
        padding: 1rem 1.5rem;
        border-radius: 1rem;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        position: relative;
        min-width: 300px;
    }

    .ai-voice-message .voice-content {
        border-bottom-left-radius: 0.25rem;
    }

    .user-voice-message .voice-content {
        border-bottom-right-radius: 0.25rem;
        background-color: #3b82f6;
        color: white;
    }

    .voice-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        margin-bottom: 0.5rem;
        font-size: 0.875rem;
        opacity: 0.8;
    }

    .voice-sender {
        font-weight: 600;
    }

    .voice-time {
        font-size: 0.75rem;
    }

    .voice-text {
        line-height: 1.6;
        margin-bottom: 1rem;
    }

    .voice-controls {
        display: flex;
        gap: 0.5rem;
        flex-wrap: wrap;
    }

    .voice-controls .btn {
        font-size: 0.8rem;
        padding: 0.25rem 0.5rem;
    }

    /* Voice Input Container */
    .voice-input-container {
        padding: 1.5rem 2rem;
        background-color: white;
        border-top: 1px solid #e9ecef;
    }

    .voice-input-wrapper {
        max-width: 1200px;
        margin: 0 auto;
        display: flex;
        flex-direction: column;
        gap: 1.5rem;
    }

    .voice-control-panel {
        display: flex;
        align-items: center;
        justify-content: center;
        gap: 2rem;
    }

    .voice-record-btn {
        width: 80px;
        height: 80px;
        border-radius: 50%;
        background-color: #10b981;
        color: white;
        border: none;
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
        cursor: pointer;
        transition: all 0.3s ease;
        font-size: 1.5rem;
        gap: 0.25rem;
    }

    .voice-record-btn:hover {
        background-color: #059669;
        transform: scale(1.05);
    }

    .voice-record-btn.recording {
        background-color: #ef4444;
        animation: pulse 1.5s infinite;
    }

    .voice-record-btn.recording:hover {
        background-color: #dc2626;
    }

    .record-text {
        font-size: 0.75rem;
        font-weight: 500;
    }

    .recording-indicator {
        display: flex;
        align-items: center;
        gap: 0.5rem;
        color: #ef4444;
        font-weight: 500;
    }

    .pulse-dot {
        width: 12px;
        height: 12px;
        background-color: #ef4444;
        border-radius: 50%;
        animation: pulse 1.5s infinite;
    }

    .voice-transcription {
        display: flex;
        flex-direction: column;
        gap: 1rem;
    }

    .transcription-textarea {
        width: 100%;
        min-height: 100px;
        padding: 1rem;
        border: 2px solid #e9ecef;
        border-radius: 0.75rem;
        font-size: 1rem;
        line-height: 1.5;
        resize: vertical;
        transition: border-color 0.2s ease;
        outline: none;
    }

    .transcription-textarea:focus {
        border-color: #10b981;
    }

    .transcription-actions {
        display: flex;
        justify-content: center;
        gap: 1rem;
    }

    /* Action Buttons */
    .action-buttons {
        padding: 1.5rem 2rem;
        background-color: white;
        border-top: 1px solid #e9ecef;
        display: flex;
        justify-content: center;
        gap: 1rem;
        flex-wrap: wrap;
    }

    .action-btn {
        padding: 0.75rem 1.5rem;
        border-radius: 0.5rem;
        font-weight: 500;
        display: flex;
        align-items: center;
        gap: 0.5rem;
        transition: all 0.2s ease;
    }

    .action-btn:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
    }

    /* Loading Spinner */
    .spin {
        animation: spin 1s linear infinite;
    }

    @@keyframes pulse {

        0%,
        100% {
            opacity: 1;
        }

        50% {
            opacity: 0.5;
        }
    }

    @@keyframes spin {
        from {
            transform: rotate(0deg);
        }

        to {
            transform: rotate(360deg);
        }
    }

    /* Responsive Design */
    @@media (max-width: 768px) {
        .header-content {
            flex-direction: column;
            gap: 1rem;
            text-align: center;
        }

        .voice-chat-container {
            padding: 1rem;
        }

        .voice-message {
            max-width: 95%;
        }

        .voice-control-panel {
            flex-direction: column;
            gap: 1rem;
        }

        .voice-record-btn {
            width: 70px;
            height: 70px;
        }

        .action-buttons {
            flex-direction: column;
            align-items: center;
        }

        .action-btn {
            width: 100%;
            max-width: 300px;
            justify-content: center;
        }
    }

    @@media (max-width: 480px) {
        .interview-header {
            padding: 1rem;
        }

        .interview-info h1 {
            font-size: 1.5rem;
        }

        .voice-chat-container {
            padding: 0.5rem;
        }

        .voice-input-container {
            padding: 1rem;
        }

        .action-buttons {
            padding: 1rem;
        }
    }
</style>

<script>
    // Voice variables
    let localStream;
    let isMuted = false;
    let isVoiceActive = false;
    let isAIVoiceEnabled = true;
    let isUserSpeaking = false;
    let speechSynthesis = window.speechSynthesis;
    let currentUtterance = null;
    let audioContext;
    let analyser;
    let microphone;
    let dataArray;
    let animationId;
    let speechRecognition;
    let isListening = false;
    let isVoiceMonitorEnabled = false;
    let voiceMonitorDestination;

    // Real-time status graph variables
    let statusGraph;
    let statusGraphCtx;
    let volumeHistory = [];
    let maxHistoryLength = 60; // 60 data points for smooth graph
    let graphAnimationId;
    let currentUserStatus = 'ready';

    // Interview flow variables
    let currentQuestionNumber = 0;
    let maxQuestions = 10;
    let isInterviewStarted = false;
    let isWaitingForUserResponse = false;

    // Interview questions
    const interviewQuestions = [
        "Hello! I'm your AI career coach. Let's start with the first question: Can you tell me about yourself and your professional background?",
        "That's great! Now, what are your key strengths and how do they contribute to your work?",
        "Excellent! Can you describe a challenging project you've worked on and how you overcame the difficulties?",
        "Very interesting! How do you stay updated with the latest trends and technologies in your field?",
        "That's impressive! Tell me about a time when you had to work with a difficult team member. How did you handle it?",
        "Good approach! What motivates you most in your professional development?",
        "Great perspective! Can you share an example of how you've mentored or helped a colleague grow?",
        "That's wonderful! How do you handle working under pressure and tight deadlines?",
        "Excellent! What are your career goals for the next 3-5 years?",
        "Perfect! Finally, do you have any questions for me about the role or the company?"
    ];

    // Initialize current time
    document.addEventListener('DOMContentLoaded', function () {
        updateCurrentTime();
        setInterval(updateCurrentTime, 1000);

        // Initialize real-time status graph
        initializeStatusGraph();

        // Initialize voice features
        initializeVoice();

        // Chat typing: enable Send when there's content and a question is active
        const transcriptionArea = document.getElementById('transcriptionArea');
        const sendBtn = document.getElementById('sendVoiceBtn');
        if (transcriptionArea && sendBtn) {
            transcriptionArea.addEventListener('input', function () {
                updateSendAvailability();
            });

            // Enter to send (Shift+Enter for newline)
            transcriptionArea.addEventListener('keydown', function (e) {
                if (e.key === 'Enter' && !e.shiftKey) {
                    e.preventDefault();
                    sendVoiceMessage();
                }
            });

            // Ensure the button is clickable regardless of inline handler
            sendBtn.setAttribute('type', 'button');
            sendBtn.addEventListener('click', function (e) {
                e.preventDefault();
                try { sendVoiceMessage(); } catch (err) { console.error('sendVoiceMessage error:', err); }
            });

            // Ensure global access for inline onclick fallback
            window.sendVoiceMessage = sendVoiceMessage;

            // Also bind direct onclick in case other scripts override listeners
            sendBtn.onclick = function (e) {
                e.preventDefault();
                try { sendVoiceMessage(); } catch (err) { console.error('sendVoiceMessage error:', err); }
            };

            // Set initial availability
            updateSendAvailability();
        }

        // Global fallback: Enter to send from anywhere when textarea focused
        window.addEventListener('keydown', function (e) {
            const active = document.activeElement;
            if (active && active.id === 'transcriptionArea' && e.key === 'Enter' && !e.shiftKey) {
                e.preventDefault();
                try { sendVoiceMessage(); } catch (err) { console.error('sendVoiceMessage error:', err); }
            }
        });
    });

    // Centralized: enable/disable Send
    function updateSendAvailability() {
        const area = document.getElementById('transcriptionArea');
        const sendBtn = document.getElementById('sendVoiceBtn');
        if (!area || !sendBtn) return;
        const hasText = area.value.trim().length > 0;
    }

    function updateCurrentTime() {
        const now = new Date();
        const timeString = now.toLocaleTimeString();
        document.getElementById('currentTime').textContent = timeString;
    }

    // Initialize real-time status graph
    function initializeStatusGraph() {
        statusGraph = document.getElementById('statusGraph');
        if (statusGraph) {
            statusGraphCtx = statusGraph.getContext('2d');
            startStatusGraphAnimation();
        }
    }

    // Start status graph animation
    function startStatusGraphAnimation() {
        function animate() {
            drawStatusGraph();
            graphAnimationId = requestAnimationFrame(animate);
        }
        animate();
    }

    // Draw real-time status graph
    function drawStatusGraph() {
        if (!statusGraphCtx) return;

        const canvas = statusGraph;
        const ctx = statusGraphCtx;
        const width = canvas.width;
        const height = canvas.height;

        // Clear canvas
        ctx.clearRect(0, 0, width, height);

        // Set graph style based on current status
        let graphColor = '#22c55e'; // Green for ready
        if (currentUserStatus === 'speaking') {
            graphColor = '#ef4444'; // Red for speaking
        } else if (currentUserStatus === 'listening') {
            graphColor = '#3b82f6'; // Blue for listening
        } else if (currentUserStatus === 'processing') {
            graphColor = '#f59e0b'; // Orange for processing
        }

        // Draw grid lines
        ctx.strokeStyle = 'rgba(255, 255, 255, 0.1)';
        ctx.lineWidth = 1;
        for (let i = 0; i <= 4; i++) {
            const y = (height / 4) * i;
            ctx.beginPath();
            ctx.moveTo(0, y);
            ctx.lineTo(width, y);
            ctx.stroke();
        }

        // Draw volume history graph
        if (volumeHistory.length > 1) {
            ctx.strokeStyle = graphColor;
            ctx.lineWidth = 2;
            ctx.beginPath();

            for (let i = 0; i < volumeHistory.length; i++) {
                const x = (width / (maxHistoryLength - 1)) * i;
                const y = height - (volumeHistory[i] / 100) * height;
                
                if (i === 0) {
                    ctx.moveTo(x, y);
                } else {
                    ctx.lineTo(x, y);
                }
            }
            ctx.stroke();

            // Add gradient fill
            const gradient = ctx.createLinearGradient(0, 0, 0, height);
            gradient.addColorStop(0, graphColor + '40');
            gradient.addColorStop(1, graphColor + '10');
            
            ctx.fillStyle = gradient;
            ctx.lineTo(width, height);
            ctx.lineTo(0, height);
            ctx.closePath();
            ctx.fill();
        }
    }

    // Update user status
    function updateUserStatus(status, volume = 0) {
        currentUserStatus = status;
        
        // Update status text and dot
        const statusText = document.getElementById('userStatus');
        const statusDot = document.getElementById('statusDot');
        
        if (statusText && statusDot) {
            // Remove all status classes
            statusDot.classList.remove('speaking', 'listening', 'processing');
            
            switch (status) {
                case 'speaking':
                    statusText.textContent = 'Speaking';
                    statusDot.classList.add('speaking');
                    break;
                case 'listening':
                    statusText.textContent = 'Listening';
                    statusDot.classList.add('listening');
                    break;
                case 'processing':
                    statusText.textContent = 'Processing';
                    statusDot.classList.add('processing');
                    break;
                default:
                    statusText.textContent = 'Ready';
                    break;
            }
        }

        // Add volume to history
        volumeHistory.push(volume);
        if (volumeHistory.length > maxHistoryLength) {
            volumeHistory.shift();
        }
    }

    // Voice Functions
    async function initializeVoice() {
        try {
            // Check if getUserMedia is supported
            if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                throw new Error('getUserMedia not supported');
            }

            // Get user media (microphone only, no video)
            localStream = await navigator.mediaDevices.getUserMedia({
                video: false,
                audio: {
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true
                }
            });
            
            // Initialize audio context for microphone monitoring
            await initializeAudioContext();
            
            // Initialize speech recognition
            initializeSpeechRecognition();
            
            addSystemMessage("Voice interview is ready. Click 'Start Recording' to begin.");
            
        } catch (error) {
            console.error('Voice initialization error:', error);
            
            // Check if it's a permission error
            if (error.name === 'NotAllowedError' || error.name === 'PermissionDeniedError') {
                addSystemMessage("Please allow microphone access for voice interview.");
            } else if (error.name === 'NotFoundError' || error.name === 'DevicesNotFoundError') {
                addSystemMessage("No microphone detected. Please connect a microphone and refresh the page.");
            } else {
                addSystemMessage("Error accessing microphone. Please check your microphone settings.");
            }
        }
    }

    // Initialize audio context for microphone monitoring
    async function initializeAudioContext() {
        try {
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            analyser = audioContext.createAnalyser();
            analyser.fftSize = 256;
            
            microphone = audioContext.createMediaStreamSource(localStream);
            microphone.connect(analyser);
            
            // Create voice monitor destination (speakers)
            voiceMonitorDestination = audioContext.destination;
            
            dataArray = new Uint8Array(analyser.frequencyBinCount);
        } catch (error) {
            console.error('Audio context initialization error:', error);
        }
    }

    // Initialize speech recognition
    function initializeSpeechRecognition() {
        try {
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            if (SpeechRecognition) {
                speechRecognition = new SpeechRecognition();
                speechRecognition.continuous = true;
                speechRecognition.interimResults = true;
                speechRecognition.lang = 'en-US'; // Can be made dynamic based on culture
                
                speechRecognition.onresult = function(event) {
                    let finalTranscript = '';
                    let interimTranscript = '';
                    
                    for (let i = event.resultIndex; i < event.results.length; i++) {
                        const transcript = event.results[i][0].transcript;
                        if (event.results[i].isFinal) {
                            finalTranscript += transcript;
                        } else {
                            interimTranscript += transcript;
                        }
                    }
                    
                    // Update transcription area with interim results
                    if (interimTranscript) {
                        document.getElementById('transcriptionArea').value = interimTranscript;
                    }
                    
                    // If we have a final result, enable send button
                    if (finalTranscript.trim()) {
                        console.log('Final transcript:', finalTranscript);
                        document.getElementById('transcriptionArea').value = finalTranscript.trim();
                        // Only enable send button if we're waiting for user response
                        if (isWaitingForUserResponse) {
                            updateSendAvailability();
                        }
                    }
                };
                
                speechRecognition.onerror = function(event) {
                    console.error('Speech recognition error:', event.error);
                    if (event.error === 'no-speech') {
                        // Restart listening if no speech detected
                        if (isListening && isVoiceActive) {
                            setTimeout(() => {
                                if (isListening && isVoiceActive) {
                                    speechRecognition.start();
                                }
                            }, 1000);
                        }
                    }
                };
                
                speechRecognition.onend = function() {
                    // Restart listening if still active
                    if (isListening && isVoiceActive) {
                        setTimeout(() => {
                            if (isListening && isVoiceActive) {
                                speechRecognition.start();
                            }
                        }, 1000);
                    }
                };
            }
        } catch (error) {
            console.error('Speech recognition initialization error:', error);
        }
    }

    // Monitor microphone activity
    function monitorMicrophone() {
        if (!analyser || !dataArray) return;
        
        analyser.getByteFrequencyData(dataArray);
        
        // Calculate average volume
        const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
        const normalizedVolume = Math.min((average / 128) * 100, 100);
        
        // Update real-time status based on volume and voice activity
        if (isVoiceActive) {
            if (average > 30) {
                if (!isUserSpeaking) {
                    isUserSpeaking = true;
                    updateUserStatus('speaking', normalizedVolume);
                    document.getElementById('recordingIndicator').style.display = 'flex';
                } else {
                    updateUserStatus('speaking', normalizedVolume);
                }
            } else {
                if (isUserSpeaking) {
                    isUserSpeaking = false;
                    updateUserStatus('listening', normalizedVolume);
                    document.getElementById('recordingIndicator').style.display = 'none';
                } else {
                    updateUserStatus('listening', normalizedVolume);
                }
            }
        } else {
            updateUserStatus('ready', normalizedVolume);
        }
        
        // Continue monitoring
        animationId = requestAnimationFrame(monitorMicrophone);
    }

    async function toggleRecording() {
        if (!isVoiceActive) {
            isVoiceActive = true;
            document.getElementById('recordButton').classList.add('recording');
            document.getElementById('recordButton').innerHTML = '<i class="bi bi-stop-fill"></i><span class="record-text">Stop</span>';
            
            // Start the interview if not already started
            if (!isInterviewStarted) {
                startInterview();
            } else {
                addSystemMessage("Voice interview resumed. You can now speak and type simultaneously.");
            }
            
            // Start microphone monitoring
            if (analyser && dataArray) {
                monitorMicrophone();
            }
            
            // Start speech recognition
            if (speechRecognition) {
                try {
                    isListening = true;
                    speechRecognition.start();
                    if (isInterviewStarted) {
                        addSystemMessage("Speech recognition started. You can now speak your answers.");
                    }
                } catch (error) {
                    console.error('Error starting speech recognition:', error);
                    addSystemMessage("Speech recognition failed to start. You can still type your answers.");
                }
            }
        } else {
            isVoiceActive = false;
            document.getElementById('recordButton').classList.remove('recording');
            document.getElementById('recordButton').innerHTML = '<i class="bi bi-mic-fill"></i><span class="record-text">Start Recording</span>';
            addSystemMessage("Voice interview paused.");
            
            // Stop microphone monitoring
            if (animationId) {
                cancelAnimationFrame(animationId);
                animationId = null;
            }
            
            // Stop speech recognition
            if (speechRecognition && isListening) {
                try {
                    isListening = false;
                    speechRecognition.stop();
                } catch (error) {
                    console.error('Error stopping speech recognition:', error);
                }
            }
            
            // Hide recording indicator
            isUserSpeaking = false;
            document.getElementById('recordingIndicator').style.display = 'none';
        }
    }

    // Speak AI message
    function speakAIMessage(message) {
        if (!isAIVoiceEnabled || !speechSynthesis) return;

        // Stop any current speech
        if (currentUtterance) {
            speechSynthesis.cancel();
        }

        // Create new utterance
        currentUtterance = new SpeechSynthesisUtterance(message);
        currentUtterance.rate = 0.9; // Slightly slower
        currentUtterance.pitch = 1.0;
        currentUtterance.volume = 0.8;

        // Try to get a good voice
        const voices = speechSynthesis.getVoices();
        const preferredVoice = voices.find(voice => 
            voice.lang.includes('en') && voice.name.includes('Google')
        ) || voices.find(voice => 
            voice.lang.includes('en')
        ) || voices[0];

        if (preferredVoice) {
            currentUtterance.voice = preferredVoice;
        }

        // Handle speech events
        currentUtterance.onstart = () => {
            console.log('AI started speaking');
        };

        currentUtterance.onend = () => {
            console.log('AI finished speaking');
            currentUtterance = null;
        };

        currentUtterance.onerror = (event) => {
            console.error('Speech synthesis error:', event);
            currentUtterance = null;
        };

        // Start speaking
        speechSynthesis.speak(currentUtterance);
    }

    // Start the interview
    function startInterview() {
        if (isInterviewStarted) return;
        
        isInterviewStarted = true;
        currentQuestionNumber = 0;
        isWaitingForUserResponse = false;
        
        addSystemMessage("Interview started! The AI will ask you questions. Please answer each question thoroughly.");
        
        // Ask the first question
        askNextQuestion();
    }

    // Ask the next question
    function askNextQuestion() {
        if (currentQuestionNumber >= maxQuestions || currentQuestionNumber >= interviewQuestions.length) {
            completeInterview();
            return;
        }

        const question = interviewQuestions[currentQuestionNumber];
        currentQuestionNumber++;
        isWaitingForUserResponse = true;

        // Add AI question to chat
        addAIMessage(question);
        
        // Speak the question
        speakAIMessage(question);
        
        // Update status
        updateUserStatus('listening', 0);

        // Focus the text input for chat answers and enable send when typing
        const transcriptionArea = document.getElementById('transcriptionArea');
        const sendBtn = document.getElementById('sendVoiceBtn');
        if (transcriptionArea && sendBtn) {
            transcriptionArea.readOnly = false;
            transcriptionArea.focus();
            // Reset value if some browsers keep previous content on reload
            if (!isWaitingForUserResponse) {
                transcriptionArea.value = '';
            }
            updateSendAvailability();
        }
    }

    // Add AI message to chat
    function addAIMessage(message) {
        const voiceMessages = document.getElementById('voiceMessages');
        const messageDiv = document.createElement('div');
        messageDiv.className = 'voice-message ai-voice-message';

        const now = new Date();
        const timeString = now.toLocaleTimeString();

        messageDiv.innerHTML = `
            <div class="voice-avatar">
                <i class="bi bi-robot"></i>
            </div>
            <div class="voice-content">
                <div class="voice-header">
                    <span class="voice-sender">AI</span>
                    <span class="voice-time">${timeString}</span>
                </div>
                <div class="voice-text">${message}</div>
            </div>
        `;

        voiceMessages.appendChild(messageDiv);
        scrollToBottom();
    }

    // Complete the interview
    function completeInterview() {
        isWaitingForUserResponse = false;
        const completionMessage = "Thank you for completing the interview! You've answered all the questions. The interview is now complete.";
        
        addAIMessage(completionMessage);
        speakAIMessage(completionMessage);
        
        addSystemMessage("Interview completed! You can review your answers or finish the interview.");
        
        // Disable the send button
        document.getElementById('transcriptionArea').readOnly = true;
        
        updateUserStatus('ready', 0);
    }

    function sendVoiceMessage() {
        try {
            const area = document.getElementById('transcriptionArea');
            if (!area) {
                console.error('transcriptionArea not found');
                addSystemMessage('Input box not ready. Please reload this page.');
                return;
            }

            const transcription = area.value.trim();
            if (!transcription) {
                // If empty but Enter pressed, do nothing
                return;
            }

            // Add user voice message to chat
            addUserVoiceMessage(transcription);

            // Clear transcription
            clearTranscription();

            // Check if we're waiting for user response
            if (isWaitingForUserResponse) {
                isWaitingForUserResponse = false;
                
                // Acknowledge the answer and ask next question
                setTimeout(() => {
                    acknowledgeAnswerAndAskNext(transcription);
                }, 500);
            } else {
                // If flow got out of sync, continue by asking the next question
                setTimeout(() => {
                    askNextQuestion();
                }, 500);
            }

            // Disable send until next question
            updateSendAvailability();
        } catch (err) {
            console.error('sendVoiceMessage fatal error:', err);
            addSystemMessage('Could not send your message. Please try again.');
        }
    }

    function addUserVoiceMessage(text) {
        const voiceMessages = document.getElementById('voiceMessages');
        const messageDiv = document.createElement('div');
        messageDiv.className = 'voice-message user-voice-message';

        const now = new Date();
        const timeString = now.toLocaleTimeString();

        messageDiv.innerHTML = `
            <div class="voice-avatar">
                <i class="bi bi-person"></i>
            </div>
            <div class="voice-content">
                <div class="voice-header">
                    <span class="voice-sender">You</span>
                    <span class="voice-time">${timeString}</span>
                </div>
                <div class="voice-text">${text}</div>
            </div>
        `;

        voiceMessages.appendChild(messageDiv);
        scrollToBottom();
    }

    // Acknowledge user answer and ask next question
    function acknowledgeAnswerAndAskNext(userAnswer) {
        // Check if this was the last question
        if (currentQuestionNumber >= maxQuestions || currentQuestionNumber >= interviewQuestions.length) {
            // This was the last question, go directly to completion
            completeInterview();
            return;
        }

        // Update status to processing
        updateUserStatus('processing', 0);

        // Go directly to the next question without acknowledgment
        setTimeout(() => {
            askNextQuestion();
        }, 1000);
    }

    async function generateAIVoiceResponse(userMessage) {
        // This function is now replaced by acknowledgeAnswerAndAskNext
        // But keeping it for backward compatibility
        acknowledgeAnswerAndAskNext(userMessage);
    }

    function playAIResponse(button) {
        // Play the AI response using text-to-speech
        const messageContent = button.closest('.voice-content');
        const text = messageContent.querySelector('.voice-text').textContent;
        speakAIMessage(text);
    }

    function pauseAIResponse(button) {
        // Pause the AI response audio
        if ('speechSynthesis' in window) {
            speechSynthesis.pause();
        }
    }

    function clearTranscription() {
        document.getElementById('transcriptionArea').value = '';
        // Only disable send button if interview is not started or completed
        if (!isInterviewStarted || !isWaitingForUserResponse) {
            updateSendAvailability();
        }
    }

    function scrollToBottom() {
        const chatContainer = document.querySelector('.voice-chat-container');
        chatContainer.scrollTop = chatContainer.scrollHeight;
    }

    function addSystemMessage(text) {
        const voiceMessages = document.getElementById('voiceMessages');
        const messageElement = document.createElement("div");
        messageElement.className = "voice-message text-center text-muted";
        messageElement.innerHTML = `<em>${text}</em>`;
        voiceMessages.appendChild(messageElement);
        scrollToBottom();
    }

    function finishInterview() {
        if (confirm('Are you sure you want to finish this interview? This action cannot be undone.')) {
            // Stop local stream
            if (localStream) {
                localStream.getTracks().forEach(track => track.stop());
            }

            // Stop any current speech
            if (currentUtterance) {
                speechSynthesis.cancel();
            }

            // Stop speech recognition
            if (speechRecognition && isListening) {
                try {
                    speechRecognition.stop();
                } catch (error) {
                    console.error('Error stopping speech recognition:', error);
                }
            }

            // Disconnect voice monitoring
            if (microphone && voiceMonitorDestination) {
                microphone.disconnect(voiceMonitorDestination);
            }

            alert('Interview finished! Redirecting to dashboard...');
            window.location.href = '/Dashboard';
        }
    }

    // Initialize speech synthesis voices
    function initializeVoices() {
        if (speechSynthesis.onvoiceschanged !== undefined) {
            speechSynthesis.onvoiceschanged = () => {
                console.log('Voices loaded:', speechSynthesis.getVoices().length);
            };
        }
    }

    // Initialize voices
    initializeVoices();

    // Cleanup on page unload
    window.addEventListener('beforeunload', () => {
        if (localStream) {
            localStream.getTracks().forEach(track => track.stop());
        }
        if (currentUtterance) {
            speechSynthesis.cancel();
        }
        if (animationId) {
            cancelAnimationFrame(animationId);
        }
        if (graphAnimationId) {
            cancelAnimationFrame(graphAnimationId);
        }
        if (audioContext) {
            audioContext.close();
        }
        if (speechRecognition && isListening) {
            try {
                speechRecognition.stop();
            } catch (error) {
                console.error('Error stopping speech recognition on cleanup:', error);
            }
        }
        // Disconnect voice monitoring
        if (microphone && voiceMonitorDestination) {
            microphone.disconnect(voiceMonitorDestination);
        }
    });
</script>

@section Scripts {
    <partial name="_ValidationScriptsPartial" />
}
