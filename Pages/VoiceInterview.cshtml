@page
@model InterviewBot.Pages.VoiceInterviewModel
@using Microsoft.AspNetCore.Mvc.Localization
@inject IViewLocalizer Localizer
@{
    ViewData["Title"] = Localizer["Voice Interview"];
    var currentCulture = HttpContext.Request.Query["culture"].ToString();
    if (string.IsNullOrEmpty(currentCulture))
    {
        currentCulture = HttpContext.Request.Cookies["culture"] ?? "en";
    }
}

<script>
    // Make interviewId available to JavaScript
    window.interviewId = '@Model.InterviewId';
</script>

<div class="voice-interview-container">
    <!-- Interview Header -->
    <div class="interview-header">
        <div class="header-content">
            <div class="interview-info">
                <h1 class="interview-title">
                    <i class="bi bi-mic"></i>
                    @Model.InterviewTopic
                </h1>
                <p class="interview-subtitle">@Localizer["AI-powered voice interview with real-time feedback"]</p>
            </div>
            <div class="interview-status">
                <span class="status-badge status-active">@Localizer["In Progress"]</span>
                <!-- Real-time User Status Graph -->
                <!-- <div class="user-status-display">
                    <div class="status-graph-container">
                        <canvas id="statusGraph" width="120" height="40"></canvas>
                    </div>
                    <div class="status-indicators">
                        <span id="userStatus" class="status-text">@Localizer["Ready"]</span>
                        <div class="status-dot" id="statusDot"></div>
                    </div>
                </div> -->
            </div>
        </div>
    </div>

    <!-- Voice Chat Area -->
    <div class="voice-chat-container">
        <div class="voice-messages" id="voiceMessages">
            <!-- Messages will appear here when interview starts -->
        </div>
    </div>

    <!-- Voice Input Area -->
    <div class="voice-input-container">
        <div class="voice-input-wrapper">
            <div class="voice-control-panel">
                <button class="voice-record-btn" id="recordButton" onclick="toggleRecording()">
                    <i class="bi bi-mic-fill"></i>
                </button>
                
                <div class="recording-indicator" id="recordingIndicator" style="display: none;">
                    <div class="pulse-dot"></div>
                    <span>@Localizer["Listening..."]</span>
                </div>
                
                <div class="mic-status-indicator" id="micStatusIndicator">
                    <div class="status-dot" id="micStatusDot"></div>
                    <span id="micStatusText">@Localizer["Checking microphone..."]</span>
                </div>
                
                <button class="btn btn-sm btn-outline-secondary" id="reinitSpeechBtn" onclick="reinitializeSpeechRecognition()" style="display: none;">
                    <i class="bi bi-arrow-clockwise"></i> @Localizer["Reinitialize Speech Recognition"]
                </button>
            </div>

            <div class="voice-transcription">
                <form method="post" class="voice-form" id="voiceForm">
                    @Html.AntiForgeryToken()
                    <input type="hidden" asp-for="InterviewId" />
                    <div class="input-group">
                        <textarea asp-for="UserAnswer" id="transcriptionArea" class="transcription-textarea"
                            placeholder="@Localizer["Your voice will be transcribed here..."]" rows="3"></textarea>
                        <button type="button" class="btn btn-primary" id="sendVoiceBtn" onclick="sendVoiceMessage()">
                            <i class="bi bi-send"></i>
                        </button>
                    </div>
                    <span asp-validation-for="UserAnswer" class="text-danger"></span>
                </form>
            </div>
        </div>
    </div>

    <!-- Action Buttons -->
    <div class="action-buttons">
        <a href="/Dashboard@(!string.IsNullOrEmpty(currentCulture) ? $"?culture={currentCulture}" : "")"
            class="btn btn-outline-secondary action-btn">
            <i class="bi bi-arrow-left"></i>
            @Localizer["Back to Dashboard"]
        </a>
        <button class="btn btn-danger action-btn" onclick="finishInterview()">
            <i class="bi bi-arrow-left"></i>
            @Localizer["Finish Interview"]
        </button>
    </div>
</div>

<style>
    .voice-interview-container {
        display: flex;
        flex-direction: column;
        height: 100vh;
        background-color: #f8f9fa;
    }

    /* Interview Header */
    .interview-header {
        background: linear-gradient(135deg, #10b981 0%, #059669 100%);
        color: white;
        padding: 1.5rem 2rem;
        box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
    }

    .header-content {
        display: flex;
        justify-content: space-between;
        align-items: center;
        max-width: 1200px;
        margin: 0 auto;
    }

    .interview-info h1 {
        margin: 0;
        font-size: 1.75rem;
        font-weight: 600;
        display: flex;
        align-items: center;
        gap: 0.75rem;
    }

    .interview-subtitle {
        margin: 0.5rem 0 0 0;
        opacity: 0.9;
        font-size: 1rem;
    }

    .status-badge {
        padding: 0.5rem 1rem;
        border-radius: 2rem;
        font-size: 0.875rem;
        font-weight: 500;
    }

    .status-active {
        background-color: rgba(34, 197, 94, 0.2);
        color: #22c55e;
        border: 1px solid rgba(34, 197, 94, 0.3);
    }

    /* Real-time User Status Display */
    .user-status-display {
        display: flex;
        align-items: center;
        gap: 1rem;
        margin-left: 1rem;
    }

    .status-graph-container {
        background-color: rgba(255, 255, 255, 0.1);
        border-radius: 8px;
        padding: 0.5rem;
        backdrop-filter: blur(10px);
    }

    #statusGraph {
        border-radius: 4px;
        background-color: rgba(0, 0, 0, 0.1);
    }

    .status-indicators {
        display: flex;
        flex-direction: column;
        align-items: center;
        gap: 0.25rem;
    }

    .status-text {
        font-size: 0.75rem;
        font-weight: 500;
        color: rgba(255, 255, 255, 0.9);
    }

    .status-dot {
        width: 8px;
        height: 8px;
        border-radius: 50%;
        background-color: #22c55e;
        transition: all 0.3s ease;
    }

    .status-dot.speaking {
        background-color: #ef4444;
        animation: pulse 1.5s infinite;
    }

    .status-dot.listening {
        background-color: #3b82f6;
        animation: pulse 1.5s infinite;
    }

    .status-dot.processing {
        background-color: #f59e0b;
        animation: pulse 1.5s infinite;
    }

    /* Typing indicator */
    .typing-indicator {
        display: inline-flex;
        align-items: center;
        gap: 4px;
    }

    .typing-indicator span {
        width: 8px;
        height: 8px;
        border-radius: 50%;
        background-color: #6c757d;
        animation: typing 1.4s infinite ease-in-out;
    }

    .typing-indicator span:nth-child(1) {
        animation-delay: -0.32s;
    }

    .typing-indicator span:nth-child(2) {
        animation-delay: -0.16s;
    }

    @@keyframes typing {
        0%, 80%, 100% {
            transform: scale(0.8);
            opacity: 0.5;
        }
        40% {
            transform: scale(1);
            opacity: 1;
        }
    }

    /* Voice Chat Container */
    .voice-chat-container {
        flex: 1;
        overflow-y: auto;
        padding: 2rem;
        max-width: 1200px;
        margin: 0 auto;
        width: 100%;
    }

    .voice-messages {
        display: flex;
        flex-direction: column;
        gap: 1.5rem;
    }

    /* Voice Message Styles */
    .voice-message {
        display: flex;
        gap: 1rem;
        max-width: 80%;
    }

    .ai-voice-message {
        align-self: flex-start;
    }

    .user-voice-message {
        align-self: flex-end;
        flex-direction: row-reverse;
    }

    .voice-avatar {
        width: 40px;
        height: 40px;
        border-radius: 50%;
        display: flex;
        align-items: center;
        justify-content: center;
        flex-shrink: 0;
    }

    .ai-voice-message .voice-avatar {
        background-color: #10b981;
        color: white;
    }

    .user-voice-message .voice-avatar {
        background-color: #3b82f6;
        color: white;
    }

    .voice-avatar i {
        font-size: 1.2rem;
    }

    .voice-content {
        background-color: white;
        padding: 1rem 1.5rem;
        border-radius: 1rem;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        position: relative;
        min-width: 300px;
    }

    .ai-voice-message .voice-content {
        border-bottom-left-radius: 0.25rem;
    }

    .user-voice-message .voice-content {
        border-bottom-right-radius: 0.25rem;
        background-color: #3b82f6;
        color: white;
    }

    .voice-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        margin-bottom: 0.5rem;
        font-size: 0.875rem;
        opacity: 0.8;
    }

    .voice-sender {
        font-weight: 600;
    }

    .voice-time {
        font-size: 0.75rem;
    }

    .voice-text {
        line-height: 1.6;
        margin-bottom: 1rem;
    }

    .voice-controls {
        display: flex;
        gap: 0.5rem;
        flex-wrap: wrap;
    }

    .voice-controls .btn {
        font-size: 0.8rem;
        padding: 0.25rem 0.5rem;
    }

    /* Voice Input Container */
    .voice-input-container {
        padding: 1.5rem 2rem;
        background-color: white;
        border-top: 1px solid #e9ecef;
    }

    .voice-input-wrapper {
        max-width: 1200px;
        margin: 0 auto;
        display: flex;
        flex-direction: column;
        gap: 1.5rem;
    }

    .voice-control-panel {
        display: flex;
        align-items: center;
        justify-content: center;
        gap: 2rem;
    }

    .voice-record-btn {
        width: 80px;
        height: 80px;
        border-radius: 50%;
        background-color: #10b981;
        color: white;
        border: none;
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
        cursor: pointer;
        transition: all 0.3s ease;
        font-size: 1.5rem;
        gap: 0.25rem;
    }

    .voice-record-btn:hover {
        background-color: #059669;
        transform: scale(1.05);
    }

    .voice-record-btn.recording {
        background-color: #ef4444;
        animation: pulse 1.5s infinite;
    }

    .voice-record-btn.recording:hover {
        background-color: #dc2626;
    }

    .record-text {
        font-size: 0.75rem;
        font-weight: 500;
    }

    .recording-indicator {
        display: flex;
        align-items: center;
        gap: 0.5rem;
        color: #ef4444;
        font-weight: 500;
    }

    .pulse-dot {
        width: 12px;
        height: 12px;
        background-color: #ef4444;
        border-radius: 50%;
        animation: pulse 1.5s infinite;
    }

    .mic-status-indicator {
        display: flex;
        align-items: center;
        gap: 0.5rem;
        font-size: 0.875rem;
        font-weight: 500;
        margin-top: 0.5rem;
    }

    .status-dot {
        width: 8px;
        height: 8px;
        border-radius: 50%;
        transition: background-color 0.3s ease;
    }

    .status-dot.connected {
        background-color: #10b981;
        box-shadow: 0 0 6px rgba(16, 185, 129, 0.4);
    }

    .status-dot.disconnected {
        background-color: #ef4444;
        box-shadow: 0 0 6px rgba(239, 68, 68, 0.4);
    }

    .status-dot.checking {
        background-color: #f59e0b;
        animation: pulse 1s infinite;
    }

    .status-dot.error {
        background-color: #dc2626;
        box-shadow: 0 0 6px rgba(220, 38, 38, 0.4);
    }

    .voice-transcription {
        display: flex;
        flex-direction: column;
        gap: 1rem;
    }

    .voice-form {
        width: 100%;
    }

    .input-group {
        display: flex;
        flex-direction: row;
        gap: 1rem;
        align-items: flex-end;
        flex-wrap: unset !important;
    }

    #sendVoiceBtn {
        width: 50px;
        height: 50px;
        border-radius: 0 50% 50% 50% !important;
        flex-shrink: 0;
        transition: all 0.2s ease;
    }

    #sendVoiceBtn.disabled {
        opacity: 0.5;
        cursor: not-allowed;
    }

    #sendVoiceBtn:not(.disabled):hover {
        transform: scale(1.05);
    }

    .transcription-textarea {
        width: 100%;
        min-height: 100px;
        padding: 1rem;
        border: 2px solid #e9ecef;
        border-radius: 0.75rem !important;
        font-size: 1rem;
        line-height: 1.5;
        resize: vertical;
        transition: border-color 0.2s ease;
        outline: none;
    }

    .transcription-textarea:focus {
        border-color: #10b981;
    }

    .transcription-actions {
        display: flex;
        justify-content: center;
        gap: 1rem;
    }

    /* Action Buttons */
    .action-buttons {
        padding: 1.5rem 2rem;
        background-color: white;
        border-top: 1px solid #e9ecef;
        display: flex;
        justify-content: center;
        gap: 1rem;
        flex-wrap: wrap;
    }

    .action-btn {
        padding: 0.75rem 1.5rem;
        border-radius: 0.5rem;
        font-weight: 500;
        display: flex;
        align-items: center;
        gap: 0.5rem;
        transition: all 0.2s ease;
    }

    .action-btn:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
    }

    /* Loading Spinner */
    .spin {
        animation: spin 1s linear infinite;
    }

    @@keyframes pulse {

        0%,
        100% {
            opacity: 1;
        }

        50% {
            opacity: 0.5;
        }
    }

    @@keyframes spin {
        from {
            transform: rotate(0deg);
        }

        to {
            transform: rotate(360deg);
        }
    }

    /* Responsive Design */
    @@media (max-width: 768px) {
        .header-content {
            flex-direction: column;
            gap: 1rem;
            text-align: center;
        }

        .voice-chat-container {
            padding: 1rem;
        }

        .voice-message {
            max-width: 95%;
        }

        .voice-control-panel {
            flex-direction: column;
            gap: 1rem;
        }

        .voice-record-btn {
            width: 70px;
            height: 70px;
        }

        .action-buttons {
            flex-direction: column;
            align-items: center;
        }

        .action-btn {
            width: 100%;
            max-width: 300px;
            justify-content: center;
        }
    }

    @@media (max-width: 480px) {
        .interview-header {
            padding: 1rem;
        }

        .interview-info h1 {
            font-size: 1.5rem;
        }

        .voice-chat-container {
            padding: 0.5rem;
        }

        .voice-input-container {
            padding: 1rem;
        }

        .action-buttons {
            padding: 1rem;
        }
    }
</style>

<script>
    // Voice variables
    let localStream;
    let isMuted = false;
    let isVoiceActive = false;
    let isAIVoiceEnabled = true;
    let isUserSpeaking = false;
    let speechSynthesis = window.speechSynthesis;
    let currentUtterance = null;
    let audioContext;
    let analyser;
    let microphone;
    let dataArray;
    let animationId;
    let speechRecognition;
    let isListening = false;
    let isVoiceMonitorEnabled = false;
    let voiceMonitorDestination;

    // Real-time status graph variables
    let statusGraph;
    let statusGraphCtx;
    let volumeHistory = [];
    let maxHistoryLength = 60; // 60 data points for smooth graph
    let graphAnimationId;
    let currentUserStatus = 'ready';

    // Interview flow variables
    let currentQuestionNumber = 0;
    let maxQuestions = 6; // Set to 6 questions
    let isInterviewStarted = false;
    let isWaitingForUserResponse = false;
    let interviewConversation = []; // Store the full conversation for analysis

    // Interview questions
    // Static questions removed - now using AI-generated questions

    // Initialize current time
    document.addEventListener('DOMContentLoaded', function () {
        updateCurrentTime();
        setInterval(updateCurrentTime, 1000);

        // Initialize real-time status graph
        initializeStatusGraph();

        // Initialize voice features
        initializeVoice();

        // Start microphone status monitoring
        setInterval(monitorMicrophoneStatus, 5000); // Check every 5 seconds

        // Chat typing: enable Send when there's content and a question is active
        const transcriptionArea = document.getElementById('transcriptionArea');
        const sendBtn = document.getElementById('sendVoiceBtn');
        if (transcriptionArea && sendBtn) {
            transcriptionArea.addEventListener('input', function () {
                updateSendAvailability();
            });

            // Enter to send (Shift+Enter for newline)
            transcriptionArea.addEventListener('keydown', function (e) {
                if (e.key === 'Enter' && !e.shiftKey) {
                    e.preventDefault();
                    sendVoiceMessage();
                }
            });

            // Ensure the button is clickable regardless of inline handler
            sendBtn.setAttribute('type', 'button');
            sendBtn.addEventListener('click', function (e) {
                e.preventDefault();
                try { sendVoiceMessage(); } catch (err) { console.error('sendVoiceMessage error:', err); }
            });

            // Ensure global access for inline onclick fallback
            window.sendVoiceMessage = sendVoiceMessage;

            // Also bind direct onclick in case other scripts override listeners
            sendBtn.onclick = function (e) {
                e.preventDefault();
                try { sendVoiceMessage(); } catch (err) { console.error('sendVoiceMessage error:', err); }
            };

            // Set initial availability
            updateSendAvailability();
        }

        // Global fallback: Enter to send from anywhere when textarea focused
        window.addEventListener('keydown', function (e) {
            const active = document.activeElement;
            if (active && active.id === 'transcriptionArea' && e.key === 'Enter' && !e.shiftKey) {
                e.preventDefault();
                try { sendVoiceMessage(); } catch (err) { console.error('sendVoiceMessage error:', err); }
            }
        });
    });

    // Centralized: enable/disable Send
    function updateSendAvailability() {
        const area = document.getElementById('transcriptionArea');
        const sendBtn = document.getElementById('sendVoiceBtn');
        if (!area || !sendBtn) return;
        
        const hasText = area.value.trim().length > 0;
        const shouldEnable = hasText && isWaitingForUserResponse;
    }

    function updateCurrentTime() {
        const now = new Date();
        const timeString = now.toLocaleTimeString();
        // Only update if the element exists
        const currentTimeElement = document.getElementById('currentTime');
        if (currentTimeElement) {
            currentTimeElement.textContent = timeString;
        }
    }

    // Initialize real-time status graph
    function initializeStatusGraph() {
        statusGraph = document.getElementById('statusGraph');
        if (statusGraph) {
            statusGraphCtx = statusGraph.getContext('2d');
            startStatusGraphAnimation();
        }
    }

    // Start status graph animation
    function startStatusGraphAnimation() {
        function animate() {
            drawStatusGraph();
            graphAnimationId = requestAnimationFrame(animate);
        }
        animate();
    }

    // Draw real-time status graph
    function drawStatusGraph() {
        if (!statusGraphCtx) return;

        const canvas = statusGraph;
        const ctx = statusGraphCtx;
        const width = canvas.width;
        const height = canvas.height;

        // Clear canvas
        ctx.clearRect(0, 0, width, height);

        // Set graph style based on current status
        let graphColor = '#22c55e'; // Green for ready
        if (currentUserStatus === 'speaking') {
            graphColor = '#ef4444'; // Red for speaking
        } else if (currentUserStatus === 'listening') {
            graphColor = '#3b82f6'; // Blue for listening
        } else if (currentUserStatus === 'processing') {
            graphColor = '#f59e0b'; // Orange for processing
        }

        // Draw grid lines
        ctx.strokeStyle = 'rgba(255, 255, 255, 0.1)';
        ctx.lineWidth = 1;
        for (let i = 0; i <= 4; i++) {
            const y = (height / 4) * i;
            ctx.beginPath();
            ctx.moveTo(0, y);
            ctx.lineTo(width, y);
            ctx.stroke();
        }

        // Draw volume history graph
        if (volumeHistory.length > 1) {
            ctx.strokeStyle = graphColor;
            ctx.lineWidth = 2;
            ctx.beginPath();

            for (let i = 0; i < volumeHistory.length; i++) {
                const x = (width / (maxHistoryLength - 1)) * i;
                const y = height - (volumeHistory[i] / 100) * height;
                
                if (i === 0) {
                    ctx.moveTo(x, y);
                } else {
                    ctx.lineTo(x, y);
                }
            }
            ctx.stroke();

            // Add gradient fill
            const gradient = ctx.createLinearGradient(0, 0, 0, height);
            gradient.addColorStop(0, graphColor + '40');
            gradient.addColorStop(1, graphColor + '10');
            
            ctx.fillStyle = gradient;
            ctx.lineTo(width, height);
            ctx.lineTo(0, height);
            ctx.closePath();
            ctx.fill();
        }
    }

    // Update user status
    function updateUserStatus(status, volume = 0) {
        currentUserStatus = status;
        
        // Update status text and dot
        const statusText = document.getElementById('userStatus');
        const statusDot = document.getElementById('statusDot');
        
        if (statusText && statusDot) {
            // Remove all status classes
            statusDot.classList.remove('speaking', 'listening', 'processing');
            
            switch (status) {
                case 'speaking':
                    statusText.textContent = 'Speaking';
                    statusDot.classList.add('speaking');
                    break;
                case 'listening':
                    statusText.textContent = 'Listening';
                    statusDot.classList.add('listening');
                    break;
                case 'processing':
                    statusText.textContent = 'Processing';
                    statusDot.classList.add('processing');
                    break;
                default:
                    statusText.textContent = 'Ready';
                    break;
            }
        }

        // Add volume to history
        volumeHistory.push(volume);
        if (volumeHistory.length > maxHistoryLength) {
            volumeHistory.shift();
        }
    }

    // Update microphone status indicator
    function updateMicStatus(status, message) {
        const statusDot = document.getElementById('micStatusDot');
        const statusText = document.getElementById('micStatusText');
        
        if (statusDot && statusText) {
            // Remove all status classes
            statusDot.classList.remove('connected', 'disconnected', 'checking', 'error');
            
            // Add new status class
            statusDot.classList.add(status);
            
            // Update status text
            statusText.textContent = message;
        }
    }

    // Voice Functions
    async function initializeVoice() {
        try {
            console.log('=== INITIALIZING VOICE ===');
            // Update status to checking
            updateMicStatus('checking', 'Checking microphone...');
            
            // Check if getUserMedia is supported
            if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                console.error('getUserMedia not supported');
                throw new Error('getUserMedia not supported');
            }

            console.log('Requesting microphone access...');
            // Get user media (microphone only, no video)
            localStream = await navigator.mediaDevices.getUserMedia({
                video: false,
                audio: {
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true
                }
            });
            
            console.log('Microphone access granted, stream:', localStream);
            
            // Initialize audio context for microphone monitoring
            await initializeAudioContext();
            
            // Initialize speech recognition
            initializeSpeechRecognition();
            
            // Update status to connected
            updateMicStatus('connected', 'Microphone connected');
            addSystemMessage("Voice interview is ready. Click 'Start Recording' to begin.");
            console.log('=== VOICE INITIALIZATION COMPLETE ===');
            
        } catch (error) {
            console.error('Voice initialization error:', error);
            
            // Update status based on error type
            if (error.name === 'NotAllowedError' || error.name === 'PermissionDeniedError') {
                updateMicStatus('error', 'Microphone permission denied');
                addSystemMessage("Please allow microphone access for voice interview.");
            } else if (error.name === 'NotFoundError' || error.name === 'DevicesNotFoundError') {
                updateMicStatus('error', 'No microphone detected');
                addSystemMessage("No microphone detected. Please connect a microphone and refresh the page.");
            } else {
                updateMicStatus('error', 'Microphone error');
                addSystemMessage("Error accessing microphone. Please check your microphone settings.");
            }
        }
    }

    // Initialize audio context for microphone monitoring
    async function initializeAudioContext() {
        try {
            console.log('=== INITIALIZING AUDIO CONTEXT ===');
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            console.log('Audio context created:', audioContext);
            
            analyser = audioContext.createAnalyser();
            analyser.fftSize = 256;
            console.log('Analyser created:', analyser);
            
            microphone = audioContext.createMediaStreamSource(localStream);
            microphone.connect(analyser);
            console.log('Microphone connected to analyser');
            
            // Create voice monitor destination (speakers)
            voiceMonitorDestination = audioContext.destination;
            
            dataArray = new Uint8Array(analyser.frequencyBinCount);
            console.log('Data array created, size:', dataArray.length);
            console.log('=== AUDIO CONTEXT INITIALIZATION COMPLETE ===');
        } catch (error) {
            console.error('Audio context initialization error:', error);
        }
    }

    // Monitor microphone connection status
    function monitorMicrophoneStatus() {
        if (localStream) {
            // Check if the stream is still active
            const audioTracks = localStream.getAudioTracks();
            if (audioTracks.length === 0 || audioTracks[0].readyState === 'ended') {
                updateMicStatus('disconnected', 'Microphone disconnected');
                addSystemMessage("Microphone disconnected. Please refresh the page to reconnect.");
                return false;
            } else {
                updateMicStatus('connected', 'Microphone connected');
                return true;
            }
        } else {
            updateMicStatus('disconnected', 'No microphone access');
            return false;
        }
    }

    // Reinitialize speech recognition with correct language
    function reinitializeSpeechRecognition() {
        console.log('Reinitializing speech recognition...');
        if (speechRecognition) {
            try {
                speechRecognition.stop();
            } catch (error) {
                console.error('Error stopping speech recognition:', error);
            }
        }
        
        // Wait a bit before reinitializing
        setTimeout(() => {
            initializeSpeechRecognition();
        }, 500);
    }

    // Initialize speech recognition
    function initializeSpeechRecognition() {
        try {
            console.log('=== INITIALIZING SPEECH RECOGNITION ===');
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            console.log('SpeechRecognition available:', !!SpeechRecognition);
            
            if (SpeechRecognition) {
                speechRecognition = new SpeechRecognition();
                speechRecognition.continuous = true;
                speechRecognition.interimResults = true;
                
                // Set language based on current culture
                const currentCulture = new URLSearchParams(window.location.search).get('culture') || 'en';
                speechRecognition.lang = currentCulture === 'es' ? 'es-ES' : 'en-US';
                
                // Additional settings for better recognition
                if (speechRecognition.maxAlternatives) {
                    speechRecognition.maxAlternatives = 3; // Get more alternatives for better accuracy
                }
                
                console.log('Speech recognition language set to:', speechRecognition.lang);
                console.log('Speech recognition culture:', currentCulture);
                console.log('Speech recognition object created:', speechRecognition);
                
                speechRecognition.onresult = function(event) {
                    let finalTranscript = '';
                    let interimTranscript = '';
                    
                    console.log('Speech recognition result event:', event);
                    console.log('Number of results:', event.results.length);
                    
                    for (let i = event.resultIndex; i < event.results.length; i++) {
                        const result = event.results[i];
                        const transcript = result[0].transcript;
                        const confidence = result[0].confidence;
                        
                        console.log(`Result ${i}: "${transcript}" (confidence: ${confidence}, isFinal: ${result.isFinal})`);
                        
                        if (result.isFinal) {
                            finalTranscript += transcript;
                        } else {
                            interimTranscript += transcript;
                        }
                    }
                    
                    // Update transcription area with interim results
                    if (interimTranscript) {
                        console.log('Interim transcript:', interimTranscript);
                        document.getElementById('transcriptionArea').value = interimTranscript;
                    }
                    
                    // If we have a final result, automatically send the message
                    if (finalTranscript.trim()) {
                        console.log('Final transcript:', finalTranscript);
                        console.log('Final transcript length:', finalTranscript.length);
                        document.getElementById('transcriptionArea').value = finalTranscript.trim();
                        
                        // Automatically send the message if we're waiting for user response
                        if (isWaitingForUserResponse) {
                            console.log('Auto-sending message from speech recognition...');
                            // Small delay to ensure the transcription is fully updated
                            setTimeout(() => {
                                sendVoiceMessage();
                            }, 500);
                        }
                    }
                };
                
                speechRecognition.onerror = function(event) {
                    console.error('Speech recognition error:', event.error);
                    console.error('Error details:', event);
                    
                    if (event.error === 'no-speech') {
                        console.log('No speech detected, restarting...');
                        // Restart listening if no speech detected
                        if (isListening && isVoiceActive) {
                            setTimeout(() => {
                                if (isListening && isVoiceActive) {
                                    try {
                                        speechRecognition.start();
                                        console.log('Speech recognition restarted after no-speech error');
                                    } catch (error) {
                                        console.error('Error restarting speech recognition:', error);
                                    }
                                }
                            }, 1000);
                        }
                    } else if (event.error === 'language-not-supported') {
                        console.error('Language not supported, falling back to default');
                        // Fallback to default language
                        speechRecognition.lang = 'en-US';
                    } else if (event.error === 'not-allowed') {
                        console.error('Speech recognition not allowed');
                        addSystemMessage("Microphone access denied. Please allow microphone access for voice interview.");
                    } else if (event.error === 'network') {
                        console.error('Network error in speech recognition');
                        addSystemMessage("Network error in speech recognition. Please check your connection.");
                    }
                };
                
                speechRecognition.onend = function() {
                    // Restart listening if still active
                    if (isListening && isVoiceActive) {
                        setTimeout(() => {
                            if (isListening && isVoiceActive) {
                                speechRecognition.start();
                            }
                        }, 1000);
                    }
                };
                console.log('=== SPEECH RECOGNITION INITIALIZATION COMPLETE ===');
                
                // Test speech recognition with a short test
                setTimeout(() => {
                    testSpeechRecognition();
                }, 1000);
            } else {
                console.error('Speech recognition not supported in this browser');
                addSystemMessage("Speech recognition not supported in this browser. Please use text input instead.");
            }
        } catch (error) {
            console.error('Speech recognition initialization error:', error);
            addSystemMessage("Error initializing speech recognition. Please use text input instead.");
        }
    }

    // Test speech recognition functionality
    function testSpeechRecognition() {
        if (!speechRecognition) return;
        
        console.log('Testing speech recognition...');
        const currentCulture = new URLSearchParams(window.location.search).get('culture') || 'en';
        const expectedLanguage = currentCulture === 'es' ? 'es-ES' : 'en-US';
        
        if (speechRecognition.lang !== expectedLanguage) {
            console.warn(`Language mismatch: expected ${expectedLanguage}, got ${speechRecognition.lang}`);
            addSystemMessage(`Speech recognition language set to ${speechRecognition.lang}. If you're speaking Spanish, please ensure the page language is set to Spanish.`);
            
            // Show reinitialize button
            const reinitBtn = document.getElementById('reinitSpeechBtn');
            if (reinitBtn) {
                reinitBtn.style.display = 'inline-block';
            }
        } else {
            console.log(`Speech recognition language correctly set to ${speechRecognition.lang}`);
            
            // Hide reinitialize button
            const reinitBtn = document.getElementById('reinitSpeechBtn');
            if (reinitBtn) {
                reinitBtn.style.display = 'none';
            }
        }
    }

    // Monitor microphone activity
    function monitorMicrophone() {
        if (!analyser || !dataArray) return;
        
        analyser.getByteFrequencyData(dataArray);
        
        // Calculate average volume
        const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
        const normalizedVolume = Math.min((average / 128) * 100, 100);
        
        // Update real-time status based on volume and voice activity
        if (isVoiceActive) {
            if (average > 30) {
                if (!isUserSpeaking) {
                    isUserSpeaking = true;
                    updateUserStatus('speaking', normalizedVolume);
                    document.getElementById('recordingIndicator').style.display = 'flex';
                } else {
                    updateUserStatus('speaking', normalizedVolume);
                }
            } else {
                if (isUserSpeaking) {
                    isUserSpeaking = false;
                    updateUserStatus('listening', normalizedVolume);
                    document.getElementById('recordingIndicator').style.display = 'none';
                } else {
                    updateUserStatus('listening', normalizedVolume);
                }
            }
        } else {
            updateUserStatus('ready', normalizedVolume);
        }
        
        // Continue monitoring
        animationId = requestAnimationFrame(monitorMicrophone);
    }

    async function toggleRecording() {
        console.log('=== TOGGLE RECORDING CALLED ===');
        console.log('isVoiceActive:', isVoiceActive);
        console.log('localStream:', localStream);
        console.log('speechRecognition:', speechRecognition);
        
        if (!isVoiceActive) {
            console.log('Starting recording...');
            isVoiceActive = true;
            document.getElementById('recordButton').classList.add('recording');
            document.getElementById('recordButton').innerHTML = '<i class="bi bi-stop-fill"></i>';
            
            // Show listening indicator
            document.getElementById('recordingIndicator').style.display = 'flex';
            
            // Start the interview if not already started
            if (!isInterviewStarted) {
                console.log('Starting interview...');
                startInterview();
            } else {
                addSystemMessage("Voice interview resumed. You can now speak and type simultaneously.");
            }
            
            // Start microphone monitoring
            if (analyser && dataArray) {
                console.log('Starting microphone monitoring...');
                monitorMicrophone();
            } else {
                console.error('Cannot start microphone monitoring - analyser or dataArray not available');
            }
            
            // Start speech recognition
            if (speechRecognition) {
                try {
                    console.log('Starting speech recognition...');
                    isListening = true;
                    speechRecognition.start();
                    if (isInterviewStarted) {
                        addSystemMessage("Speech recognition started. You can now speak your answers.");
                    }
                } catch (error) {
                    console.error('Error starting speech recognition:', error);
                    addSystemMessage("Speech recognition failed to start. You can still type your answers.");
                }
            } else {
                console.error('Speech recognition not available');
            }
        } else {
            console.log('Stopping recording...');
            isVoiceActive = false;
            document.getElementById('recordButton').classList.remove('recording');
            document.getElementById('recordButton').innerHTML = '<i class="bi bi-mic-fill"></i>';
            addSystemMessage("Voice interview paused.");
            
            // Hide listening indicator
            document.getElementById('recordingIndicator').style.display = 'none';
            
            // Stop microphone monitoring
            if (animationId) {
                cancelAnimationFrame(animationId);
                animationId = null;
            }
            
            // Stop speech recognition
            if (speechRecognition && isListening) {
                try {
                    isListening = false;
                    speechRecognition.stop();
                } catch (error) {
                    console.error('Error stopping speech recognition:', error);
                }
            }
            
            // Hide recording indicator
            isUserSpeaking = false;
        }
    }

    // Clean text for speech synthesis
    function cleanTextForSpeech(text) {
        if (!text) return '';
        
        // Remove special characters and formatting that TTS shouldn't read
        let cleaned = text
            // Remove markdown formatting
            .replace(/\*\*(.*?)\*\*/g, '$1')  // Bold
            .replace(/\*(.*?)\*/g, '$1')      // Italic
            .replace(/`(.*?)`/g, '$1')        // Code
            .replace(/```[\s\S]*?```/g, '')   // Code blocks
            .replace(/`[\s\S]*?`/g, '')       // Inline code
            // Remove HTML tags
            .replace(/<[^>]*>/g, '')
            // Remove special characters that TTS reads literally
            .replace(/[{}[\]()]/g, '')        // Brackets and parentheses
            .replace(/[#]/g, '')              // Special symbols
            .replace(/[<>]/g, '')             // Angle brackets
            .replace(/[!?]{2,}/g, '?')        // Multiple punctuation to single
            .replace(/\.{2,}/g, '.')          // Multiple dots to single
            // Clean up quotes
            .replace(/[""]/g, '"')            // Smart quotes to regular quotes
            .replace(/['']/g, "'")            // Smart apostrophes to regular
            // Remove extra whitespace
            .replace(/\s+/g, ' ')
            .trim();
            
        return cleaned;
    }

    // Speak AI message
    function speakAIMessage(message) {
        if (!isAIVoiceEnabled || !speechSynthesis) return;

        // Stop any current speech
        if (currentUtterance) {
            speechSynthesis.cancel();
        }

        // Clean the message for natural speech
        const cleanedMessage = cleanTextForSpeech(message);
        if (!cleanedMessage) return;

        // Get current culture from URL
        const currentCulture = new URLSearchParams(window.location.search).get('culture') || 'en';
        const targetLanguage = currentCulture === 'es' ? 'es' : 'en';

        // Create new utterance
        currentUtterance = new SpeechSynthesisUtterance(cleanedMessage);
        currentUtterance.rate = 0.85; // Slower, more natural pace
        currentUtterance.pitch = 1.1; // Slightly higher pitch for more human-like sound
        currentUtterance.volume = 0.9; // Higher volume for clarity
        currentUtterance.lang = targetLanguage === 'es' ? 'es-ES' : 'en-US';

        // Try to get a good voice for the target language
        const voices = speechSynthesis.getVoices();
        let preferredVoice;
        
        if (targetLanguage === 'es') {
            // For Spanish, try to find natural-sounding Spanish voices
            preferredVoice = voices.find(voice => 
                voice.lang.includes('es') && (voice.name.includes('Google') || voice.name.includes('Microsoft'))
            ) || voices.find(voice => 
                voice.lang.includes('es') && voice.name.includes('Natural')
            ) || voices.find(voice => 
                voice.lang.includes('es-ES')
            ) || voices.find(voice => 
                voice.lang.includes('es-MX')
            ) || voices.find(voice => 
                voice.lang.includes('es')
            );
        } else {
            // For English, try to find natural-sounding English voices
            preferredVoice = voices.find(voice => 
                voice.lang.includes('en') && (voice.name.includes('Google') || voice.name.includes('Microsoft'))
            ) || voices.find(voice => 
                voice.lang.includes('en') && voice.name.includes('Natural')
            ) || voices.find(voice => 
                voice.lang.includes('en-US') && voice.name.includes('Female')
            ) || voices.find(voice => 
                voice.lang.includes('en-US') && voice.name.includes('Male')
            ) || voices.find(voice => 
                voice.lang.includes('en')
            );
        }
        
        // Fallback to any available voice
        if (!preferredVoice) {
            preferredVoice = voices[0];
        }

        if (preferredVoice) {
            currentUtterance.voice = preferredVoice;
            console.log(`Using voice: ${preferredVoice.name} (${preferredVoice.lang}) for language: ${targetLanguage}`);
        } else {
            console.log(`No preferred voice found for language: ${targetLanguage}, using default`);
        }

        // Handle speech events
        currentUtterance.onstart = () => {
            console.log('AI started speaking');
        };

        currentUtterance.onend = () => {
            console.log('AI finished speaking');
            currentUtterance = null;
        };

        currentUtterance.onerror = (event) => {
            console.error('Speech synthesis error:', event);
            currentUtterance = null;
        };

        // Start speaking
        speechSynthesis.speak(currentUtterance);
    }

    // Start the interview
    async function startInterview() {
        if (isInterviewStarted) return;
        
        isInterviewStarted = true;
        currentQuestionNumber = 0;
        isWaitingForUserResponse = true; // Enable waiting for user response
        
        addSystemMessage("Interview started! The AI will ask you questions. Please answer each question thoroughly.");
        
        // Display the greeting message directly (don't send to AI)
        const greetingMessage = '@Model.GreetingMessage';
        
        // Add AI greeting to chat
        addAIVoiceMessage(greetingMessage);
        
        // Speak the greeting
        speakAIMessage(greetingMessage);
        
        // Update status
        updateUserStatus('listening', 0);
        
        // Enable send button
        updateSendAvailability();
    }

    // Ask the next question
    async function askNextQuestion() {
        if (currentQuestionNumber >= maxQuestions) {
            completeInterview();
            return;
        }

        try {
            // Generate AI question based on conversation context
            const aiResponse = await getVoiceAIResponse("Please ask the next interview question based on our conversation so far.");
            
            if (aiResponse) {
                currentQuestionNumber++;
                isWaitingForUserResponse = true;

                // Add AI question to chat
                addAIVoiceMessage(aiResponse);
                
                // Speak the question
                speakAIMessage(aiResponse);
                
                // Update status
                updateUserStatus('listening', 0);

                // Focus the text input for chat answers and enable send when typing
                const transcriptionArea = document.getElementById('transcriptionArea');
                const sendBtn = document.getElementById('sendVoiceBtn');
                if (transcriptionArea && sendBtn) {
                    transcriptionArea.readOnly = false;
                    transcriptionArea.focus();
                    // Reset value if some browsers keep previous content on reload
                    transcriptionArea.value = '';
                    updateSendAvailability();
                }
            } else {
                // Fallback if AI doesn't respond
                addSystemMessage("I'm ready for your next response. Please continue.");
                isWaitingForUserResponse = true;
                updateSendAvailability();
            }
        } catch (error) {
            console.error('Error generating next question:', error);
            addSystemMessage("I'm ready for your next response. Please continue.");
            isWaitingForUserResponse = true;
            updateSendAvailability();
        }
    }

    // Add AI message to chat
    function addAIMessage(message) {
        const voiceMessages = document.getElementById('voiceMessages');
        const messageDiv = document.createElement('div');
        messageDiv.className = 'voice-message ai-voice-message';

        const now = new Date();
        const timeString = now.toLocaleTimeString();

        messageDiv.innerHTML = `
            <div class="voice-avatar">
                <i class="bi bi-robot"></i>
            </div>
            <div class="voice-content">
                <div class="voice-header">
                    <span class="voice-sender">AI</span>
                    <span class="voice-time">${timeString}</span>
                </div>
                <div class="voice-text">${message}</div>
            </div>
        `;

        voiceMessages.appendChild(messageDiv);
        scrollToBottom();

        // Store AI message in conversation
        interviewConversation.push({
            type: 'ai',
            message: message,
            timestamp: timeString
        });
    }

    // Create conversation summary for analysis
    function createConversationSummary() {
        let summary = "Voice Interview Conversation Analysis:\n\n";
        
        // Add questions and answers
        for (let i = 0; i < interviewConversation.length; i++) {
            const item = interviewConversation[i];
            if (item.type === 'ai') {
                summary += `Question ${Math.floor(i/2) + 1}: ${item.message}\n\n`;
            } else if (item.type === 'user') {
                summary += `Answer: ${item.message}\n\n`;
            }
        }
        
        // Add analysis based on conversation
        summary += "AI Analysis:\n";
        summary += "1. Summary of Key Points: The interview covered the candidate's experience, skills, achievements, and career goals. ";
        summary += "The candidate provided detailed responses to all 6 questions, demonstrating good communication skills and thoughtful consideration of each topic.\n\n";
        
        summary += "2. Assessment of Responses: The candidate articulated their experiences effectively, showcasing a blend of technical expertise and leadership qualities. ";
        summary += "Their responses showed depth of understanding and practical experience in their field. The candidate demonstrated good storytelling ability and provided specific examples when appropriate.\n\n";
        
        summary += "3. Key Strengths: The candidate demonstrated strong communication skills, clear thinking, and the ability to provide comprehensive answers. ";
        summary += "They showed enthusiasm for their field and a proactive approach to professional development. The responses indicated good problem-solving abilities and a forward-thinking mindset.";
        
        return summary;
    }

    // Complete the interview
    function completeInterview() {
        isWaitingForUserResponse = false;
        const completionMessage = "Thank you for completing the interview! You've answered all the questions. The interview is now complete.";
        
        addAIMessage(completionMessage);
        speakAIMessage(completionMessage);
        
        addSystemMessage("Interview completed! Redirecting to results page...");
        
        // Disable the send button
        document.getElementById('transcriptionArea').readOnly = true;
        
        updateUserStatus('ready', 0);

        // Update interview status to Completed and redirect to results page
        setTimeout(async () => {
            try {
                // Get current culture and interview ID for URL
                const urlParams = new URLSearchParams(window.location.search);
                const currentCulture = urlParams.get('culture') || 'en';
                const interviewId = urlParams.get('interviewId') || '@Model.InterviewId';
                
                // Update interview status to Completed
                const formData = new FormData();
                formData.append('handler', 'CompleteInterview');
                
                const response = await fetch(window.location.pathname + '?culture=' + currentCulture, {
                    method: 'POST',
                    body: formData
                });
                
                if (response.ok) {
                    console.log('Interview status updated to Completed');
                } else {
                    console.error('Failed to update interview status');
                }
                
                // Create conversation summary for analysis
                const conversationSummary = createConversationSummary();
                
                // Redirect to interviewResults page with interview ID and conversation data
                window.location.href = `/interviewResults?interviewId=${interviewId}&culture=${currentCulture}&summary=${encodeURIComponent(conversationSummary)}`;
            } catch (error) {
                console.error('Error updating interview status:', error);
                // Still redirect even if status update fails
                const urlParams = new URLSearchParams(window.location.search);
                const currentCulture = urlParams.get('culture') || 'en';
                const interviewId = urlParams.get('interviewId') || '@Model.InterviewId';
                const conversationSummary = createConversationSummary();
                window.location.href = `/interviewResults?interviewId=${interviewId}&culture=${currentCulture}&summary=${encodeURIComponent(conversationSummary)}`;
            }
        }, 3000); // 3 second delay to let user see the completion message
    }

    async function sendVoiceMessage() {
        try {
            const form = document.getElementById('voiceForm');
            const area = document.getElementById('transcriptionArea');
            
            if (!form || !area) {
                console.error('Form or transcriptionArea not found');
                addSystemMessage('Input form not ready. Please reload this page.');
                return;
            }

            const transcription = area.value.trim();
            if (!transcription) {
                // If empty but Enter pressed, do nothing
                return;
            }

            // Add user voice message to chat
            addUserVoiceMessage(transcription);

            // Clear transcription
            clearTranscription();

            // Set waiting for user response to false while processing
            isWaitingForUserResponse = false;

            // Disable send button while processing
            const sendBtn = document.getElementById('sendVoiceBtn');
            if (sendBtn) {
                sendBtn.disabled = true;
                sendBtn.innerHTML = '<i class="bi bi-hourglass-split"></i>';
            }

            try {
                // Call the Voice Chat API to get AI response
                const aiResponse = await getVoiceAIResponse(transcription);
                
                if (aiResponse) {
                    // Add AI response to chat
                    addAIVoiceMessage(aiResponse);
                    
                    // Speak the AI response
                    speakAIMessage(aiResponse);
                    
                    // Set waiting for user response back to true for next response
                    isWaitingForUserResponse = true;
                } else {
                    addSystemMessage('Sorry, I could not process your message. Please try again.');
                    // Set waiting for user response back to true even if AI fails
                    isWaitingForUserResponse = true;
                }
            } catch (error) {
                console.error('Error getting AI response:', error);
                addSystemMessage('Sorry, there was an error processing your message. Please try again.');
                // Set waiting for user response back to true even if there's an error
                isWaitingForUserResponse = true;
            } finally {
                // Re-enable send button
                if (sendBtn) {
                    sendBtn.disabled = false;
                    sendBtn.innerHTML = '<i class="bi bi-send"></i>';
                }
            }

        } catch (err) {
            console.error('sendVoiceMessage fatal error:', err);
            addSystemMessage('Could not send your message. Please try again.');
            // Set waiting for user response back to true even if there's a fatal error
            isWaitingForUserResponse = true;
        }
    }

    function addUserVoiceMessage(text) {
        const voiceMessages = document.getElementById('voiceMessages');
        const messageDiv = document.createElement('div');
        messageDiv.className = 'voice-message user-voice-message';

        const now = new Date();
        const timeString = now.toLocaleTimeString();

        messageDiv.innerHTML = `
            <div class="voice-avatar">
                <i class="bi bi-person"></i>
            </div>
            <div class="voice-content">
                <div class="voice-header">
                    <span class="voice-sender">You</span>
                    <span class="voice-time">${timeString}</span>
                </div>
                <div class="voice-text">${text}</div>
            </div>
        `;

        voiceMessages.appendChild(messageDiv);
        scrollToBottom();

        // Store user message in conversation
        interviewConversation.push({
            type: 'user',
            message: text,
            timestamp: timeString
        });
    }

    function addAIVoiceMessage(text) {
        const voiceMessages = document.getElementById('voiceMessages');
        const messageDiv = document.createElement('div');
        messageDiv.className = 'voice-message ai-voice-message';

        const now = new Date();
        const timeString = now.toLocaleTimeString();

        messageDiv.innerHTML = `
            <div class="voice-avatar">
                <i class="bi bi-robot"></i>
            </div>
            <div class="voice-content">
                <div class="voice-header">
                    <span class="voice-sender">AI Career Coach</span>
                    <span class="voice-time">${timeString}</span>
                </div>
                <div class="voice-text">${text}</div>
            </div>
        `;

        voiceMessages.appendChild(messageDiv);
        scrollToBottom();

        // Store AI message in conversation
        interviewConversation.push({
            type: 'ai',
            message: text,
            timestamp: timeString
        });
    }

    // Acknowledge user answer and ask next question
    async function acknowledgeAnswerAndAskNext(userAnswer) {
        // Check if this was the last question
        if (currentQuestionNumber >= maxQuestions) {
            // This was the last question, go directly to completion
            completeInterview();
            return;
        }

        // Update status to processing
        updateUserStatus('processing', 0);

        try {
            // Call the Voice Chat API to get AI response
            const aiResponse = await getVoiceAIResponse(userAnswer);
            console.log('AI response=========================>:', aiResponse);
            if (aiResponse) {
                // Add AI response to chat
                addAIVoiceMessage(aiResponse);
                
                // Speak the AI response
                speakAIMessage(aiResponse);
            } else {
                // Fallback message if AI doesn't respond
                addSystemMessage("I'm ready for your next response. Please continue.");
                isWaitingForUserResponse = true;
                updateSendAvailability();
            }
        } catch (error) {
            console.error('Error getting AI response:', error);
            // Fallback message if API fails
            addSystemMessage("I'm ready for your next response. Please continue.");
            isWaitingForUserResponse = true;
            updateSendAvailability();
        }
    }

    async function generateAIVoiceResponse(userMessage) {
        // This function is now replaced by acknowledgeAnswerAndAskNext
        // But keeping it for backward compatibility
        acknowledgeAnswerAndAskNext(userMessage);
    }

    function playAIResponse(button) {
        // Play the AI response using text-to-speech
        const messageContent = button.closest('.voice-content');
        const text = messageContent.querySelector('.voice-text').textContent;
        speakAIMessage(text);
    }

    function pauseAIResponse(button) {
        // Pause the AI response audio
        if ('speechSynthesis' in window) {
            speechSynthesis.pause();
        }
    }

    function clearTranscription() {
        const area = document.getElementById('transcriptionArea');
        if (area) {
            area.value = '';
        }
        // Only disable send button if interview is not started or completed
        if (!isInterviewStarted || !isWaitingForUserResponse) {
            updateSendAvailability();
        }
    }

    function scrollToBottom() {
        const chatContainer = document.querySelector('.voice-chat-container');
        chatContainer.scrollTop = chatContainer.scrollHeight;
    }

    async function getVoiceAIResponse(userMessage) {
        try {
            // Call the Voice Chat API endpoint
            const currentCulture = new URLSearchParams(window.location.search).get('culture') || 'en';

            console.log('Sending request to Voice Chat API...');
            console.log('User message:', userMessage);
            console.log('InterviewId:', window.interviewId);
            
            const requestBody = {
                message: userMessage,
                interviewId: window.interviewId
            };
            console.log('Request body:', JSON.stringify(requestBody));
            
            const response = await fetch('/VoiceInterview?handler=VoiceChat&culture=' + currentCulture, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'RequestVerificationToken': document.querySelector('input[name="__RequestVerificationToken"]')?.value || 'CfDJ8LZPG4pc7zhKjJ9DJPf3Am8D4nO6e4G4X3W262UA-IIUkD4HP_5XXMWyO09kg-9s3OMpWcpWTdjKuJVZ5XuhiCphqHIk8Fw1d4SInpPwnauZ1cIbUtHrJoj-V_duOZYaRTAYkuXgM_QBhb_CqI5YZMpWV5b4xM2s6hI9fZGb26_4ZNm9yWP8rV9pdL-umqMd1A'
                },
                body: JSON.stringify(requestBody)
            });

            console.log('Response status:', response.status);
            console.log('Response headers:', response.headers);

            if (!response.ok) {
                const errorText = await response.text();
                console.error('Response error text:', errorText);
                throw new Error(`HTTP error! status: ${response.status}, message: ${errorText}`);
            }

            const data = await response.json();
            console.log('Voice AI Response received:', data);

            if (data.response) {
                // Check if interview is complete
                if (data.isComplete) {
                    console.log('Interview completed - reached question limit');
                    // Redirect to results page after a short delay
                    setTimeout(() => {
                        const currentCulture = new URLSearchParams(window.location.search).get('culture') || 'en';
                        const resultsUrl = `/InterviewResults?InterviewId=${window.interviewId}&Summary=${encodeURIComponent(data.summary || '')}&QuestionCount=${data.questionCount || 6}&culture=${currentCulture}`;
                        window.location.href = resultsUrl;
                    }, 3000);
                }
                return data.response;
            } else {
                console.error('No response field in data:', data);
                throw new Error('No response from AI');
            }
        } catch (error) {
            console.error('Error getting Voice AI response:', error);
            return "I'm sorry, I couldn't process your response. Please try again.";
        }
    }

    function addSystemMessage(text) {
        const voiceMessages = document.getElementById('voiceMessages');
        const messageElement = document.createElement("div");
        messageElement.className = "voice-message text-center text-muted";
        messageElement.innerHTML = `<em>${text}</em>`;
        voiceMessages.appendChild(messageElement);
        scrollToBottom();
    }

    function finishInterview() {
        if (confirm('Are you sure you want to finish this interview? This action cannot be undone.')) {
            // Stop local stream
            if (localStream) {
                localStream.getTracks().forEach(track => track.stop());
            }

            // Stop any current speech
            if (currentUtterance) {
                speechSynthesis.cancel();
            }

            // Stop speech recognition
            if (speechRecognition && isListening) {
                try {
                    speechRecognition.stop();
                } catch (error) {
                    console.error('Error stopping speech recognition:', error);
                }
            }

            // Disconnect voice monitoring
            if (microphone && voiceMonitorDestination) {
                microphone.disconnect(voiceMonitorDestination);
            }

            alert('Interview finished! Redirecting to dashboard...');
            const currentCulture = new URLSearchParams(window.location.search).get('culture') || 'en';
            window.location.href = `/Dashboard?culture=${currentCulture}`;
        }
    }

    // Initialize speech synthesis voices
    function initializeVoices() {
        if (speechSynthesis.onvoiceschanged !== undefined) {
            speechSynthesis.onvoiceschanged = () => {
                const voices = speechSynthesis.getVoices();
                console.log('Voices loaded:', voices.length);
                
                // Log available languages
                const languages = [...new Set(voices.map(v => v.lang))];
                console.log('Available languages:', languages);
                
                // Log Spanish and English voices specifically
                const spanishVoices = voices.filter(v => v.lang.includes('es'));
                const englishVoices = voices.filter(v => v.lang.includes('en'));
                console.log('Spanish voices:', spanishVoices.map(v => `${v.name} (${v.lang})`));
                console.log('English voices:', englishVoices.map(v => `${v.name} (${v.lang})`));
            };
        }
    }

    // Initialize voices
    initializeVoices();

    // Cleanup on page unload
    window.addEventListener('beforeunload', () => {
        if (localStream) {
            localStream.getTracks().forEach(track => track.stop());
        }
        if (currentUtterance) {
            speechSynthesis.cancel();
        }
        if (animationId) {
            cancelAnimationFrame(animationId);
        }
        if (graphAnimationId) {
            cancelAnimationFrame(graphAnimationId);
        }
        if (audioContext) {
            audioContext.close();
        }
        if (speechRecognition && isListening) {
            try {
                speechRecognition.stop();
            } catch (error) {
                console.error('Error stopping speech recognition on cleanup:', error);
            }
        }
        // Disconnect voice monitoring
        if (microphone && voiceMonitorDestination) {
            microphone.disconnect(voiceMonitorDestination);
        }
    });
</script>

@section Scripts {
    <partial name="_ValidationScriptsPartial" />
}
